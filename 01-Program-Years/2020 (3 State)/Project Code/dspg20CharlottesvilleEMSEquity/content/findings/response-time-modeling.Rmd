---
title: "Response Time Analysis"
description: "Do EMS response times vary throughout the region?"
weight: 3
draft: false
output: html_document
---

```{r, echo = FALSE}
# setup
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 15, fig.height = 10)

```

```{r}
load(here::here("data", "working", "model_objects", "basic_model_bayes_no_interact.RData"))
load(here::here("data", "working", "model_objects", "basic_model_bayes_yes_interact.RData"))
load(here::here("data", "working", "model_objects", "basic_model_freq_no_interact.RData"))
load(here::here("data", "working", "model_objects", "basic_model_freq_yes_interact.RData"))
load(here::here("data", "working", "model_objects", "neighbor_model_bayes_no_interact.RData"))
load(here::here("data", "working", "model_objects", "neighbor_model_bayes_yes_interact.RData"))

load(here::here("data", "working", "model_objects", "local_morans_statistic.RData"))

library(rstanarm)
library(bayesplot)
library(dplyr)
library(ggplot2)
library(sf)
library(broom)
library(purrr)
library(glue)
library(stringr)
library(viridis)
library(spdep)
library(ape)
library(leaflet)
library(leaflet.mapboxgl)
library(ggthemes)
library(tidyr)
library(glue)

prepared_data <- readr::read_csv(here::here("data", "final", "response_time_model_data_prepared.csv"))
prepared_data_sp <- sf::st_read(here::here("data", "final", "response_time_model_data_prepared_sp.geojson"), quiet = TRUE)


freq_models <- list("basic_model_freq_no_interact" = basic_model_freq_no_interact,
                    "basic_model_freq_yes_interact" = basic_model_freq_yes_interact)

bayes_models <- stanreg_list("basic_model_bayes_no_interact" = basic_model_bayes_no_interact,
                             "basic_model_bayes_yes_interact" = basic_model_bayes_yes_interact,
                             "neighbor_model_bayes_no_interact" = neighbor_model_bayes_no_interact,
                             "neighbor_model_bayes_yes_interact" = neighbor_model_bayes_yes_interact)



bayes_model_coefs <- map(bayes_models, ~tidy(.x$stanfit,
                                             estimate.method = "median",
                                             conf.int = TRUE,
                                             conf.level = 0.95)) %>%
  map(~filter(.x, !(term %in% c("sigma", "mean_PPD", "log-posterior"))))


bayes_model_coefs_trans <- bayes_model_coefs %>%
  map(~mutate(.x, across(c(estimate,
                           conf.low,
                           conf.high),
                         list(scale_factor = ~exp(.x * (.x != .x[1])),
                              time_to_incident = ~exp(.x * (.x != .x[1]) + .x[1])))))


bayes_residuals <- map(bayes_models, residuals)

augmented_data <- prepared_data_sp %>% 
  mutate(resid_basic_no = bayes_residuals$basic_model_bayes_no_interact,
         resid_basic_yes = bayes_residuals$basic_model_bayes_yes_interact,
         resid_neighbor_no = bayes_residuals$neighbor_model_bayes_no_interact,
         resid_neighbor_yes = bayes_residuals$neighbor_model_bayes_yes_interact)

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

term_dictionary <- tribble(~term, ~term_pretty,
                           "time_of_day", "Time of Day",
                           "response_vehicle_type_collapsedother", "Vehicle Type: Other",
                           "response_vehicle_type_collapsedmissing", "Vehicle Type: Missing",
                           "response_vehicle_type_collapsedfire apparatus", "Vehicle Type: Fire Apparatus",
                           "possible_impression_category_collapsedrespiratory", "Symptom Type: Respiratory",
                           "possible_impression_category_collapsedpain", "Symptom Type: Pain",
                           "possible_impression_category_collapsedother", "Symptom Type: Other",
                           "possible_impression_category_collapsedbehavioral", "Symptom Type: Behavioral",
                           "possible_impression_category_collapsedneuro", "Symptom Type: Neuro",
                           "possible_impression_category_collapsedmissing", "Symptom Type: Missing",
                           "possible_impression_category_collapsedinjury", "Symptom Type: Injury",
                           "possible_impression_category_collapsedinfectious", "Symptom Type: Infectious",
                           "possible_impression_category_collapsedgi/gu", "Symptom Type: GI/GU",
                           "possible_impression_category_collapsedendocrine", "Symptom Type: Endocrine",
                           "possible_impression_category_collapsedcv", "Symptom Type: Cardiovascular",
                           "patient_gendermissing", "Patient Gender: Missing or Unknown",
                           "patient_gendermale", "Patient Gender: Male",
                           "patient_first_race_collapsedwhite", "Patient Race: White",
                           "patient_first_race_collapsedother", "Patient Race: Other",
                           "patient_first_race_collapsedmissing", "Patient Race: Missing",
                           "patient_age", "Patient Age",
                           "after_covidTRUE", "During Covid-19",
                           "(Intercept)", "Intercept")


#sf_map_colors <- c('#7b3294','#c2a5cf','#d7d7d7','#a6dba0','#008837') 
#sf_map_colors <- c("#a52748", "#e47961", "#ffffe0",'#579eb9','#1c5796') 
sf_map_colors <- rev(c("#cc415a", "#ed9ab1", "#EBE8E8",'#92b2df','#2f75b3'))


theme_set(theme_minimal() +
            theme(plot.title = element_text(hjust = 0.5, color = "gray10", size = 22),
                  plot.subtitle = element_text(hjust = 0.5, color = "gray30", face = "italic", size = 18),
                  axis.title = element_text(size = 18, color = "gray10"),
                  axis.text = element_text(size = 16, color = "gray30"),
                  strip.text = element_text(size = 20, color = "gray30"),
                  panel.spacing = unit(4, "lines"),
                  legend.key.size = unit(3, "line"),
                  legend.text = element_text(size = 14, color = "gray30"),
                  legend.title = element_text(size = 20, color = "gray10")))

options(mapbox.accessToken = Sys.getenv("MAPBOX_TOKEN"))
```

```{r}
# fixing after covid coefs
neighbor_coef_df <- as.data.frame(neighbor_model_bayes_yes_interact)


fixed_coefs <- neighbor_coef_df %>% 
  map2_df(.x = .[3:22], .y = .[23:42], .f = ~(.x + .y + neighbor_coef_df$after_covidTRUE)) %>% 
  mutate(across(everything(), exp)) %>% 
  rename_all(~paste0("after_covidTRUE:", .x)) %>% 
  pivot_longer(everything(), names_to = "term") %>% 
  group_by(term) %>% 
  summarise(across(value, 
                   .fns = list(estimate = median, 
                               conf.low = ~quantile(.x, probs = 0.05), 
                               conf.high = ~quantile(.x, probs = 0.95)),
                   .names = "{fn}_scale_factor"),
            .groups = "drop")

bayes_model_coefs_trans$neighbor_model_bayes_yes_interact <- bayes_model_coefs_trans$neighbor_model_bayes_yes_interact %>% 
  filter(!str_detect(term, "after_covidTRUE")) %>% 
  bind_rows(fixed_coefs)
```


### Model Results


For more details on the specification of our model, see the [Data & Methods](https://dspg-young-scholars-program.github.io/dspg20CharlottesvilleEMSEquity/methods/) tab.

<br>

#### Measures of Model Fit

We assessed the specification of our model using a posterior predictive check, which is common in Bayesian estimation. The posterior predictive check compares the distribution of predicted response times for each MCMC draw, where each draw represents one guess of what the model coefficients could be, to the actual distribution of response times in the data. Below, we have the posterior predictive check results for the model including neighborhood effects. Light blue lines represent the distributions of response times predicted by the model, while dark blue lines represent the actual response times. The predictions follow the model closely, giving strong support to the strength of this model.

<br>

```{r}
pp_check(neighbor_model_bayes_yes_interact) +
  labs(x = "Log(Response Time)")
```

<br>

In our case, the posterior predictive check did not immediately favor one model over the other. To compare our two models, we can check against information criterion. These summaries indicate a model’s ability to predict data not included in the sample the model was trained on. Strong performance on this check suggests that a model represents the data generation process well.

We assessed our models using the leave-one-out information criterion, which measures how well a model predicts each data point if that data point were removed from the data the model was trained on. In the following table, the model with a 0 indicates the best fit. Large negative numbers indicate that model performed much worse than the best model. Thus, while both models produced a reasonable distribution of predicted response times, the multilevel model was superior in predicting individual response times based on the specified predictors.

<br>

```{r}
load(here::here("data", "working", "model_objects", "loo_list.RData"))

loo_table <- loo_compare(loo_list)

loo_tibble <- tibble(Model = c("Neighborhood Level Model", "Linear Model"),
                     `Expected Log Predictive Density Difference` = c(loo_table[1,1], loo_table[4,1]),
                     `Standard Error` = c(loo_table[1,2], loo_table[4,2]))

knitr::kable(loo_tibble, format = "html") %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "bordered"), full_width = FALSE)
```

<br>

#### Spatial Autocorrelation

Because our data varies over space, we must attempt to account for spatial autocorrelation, or the general tendency for data points near each other to have similar values. Many models assume that no spatial autocorrelation exists, so its presence can produce misleading results. One way to assess this is to examine the model’s prediction accuracy across space. If we see that nearby areas have similar prediction errors, we have reason to be concerned about spatial autocorrelation. We therefore plotted the residuals from the neighborhood-level model across the EMS response region.

<br>

```{r}
col_breaks <- c(-2, -0.5, -0.2, 0.2, 0.5, 2)

augmented_data %>% 
  ggplot() +
  stat_summary_hex(aes(x = scene_gps_longitude, 
                       y = scene_gps_latitude,
                       z = resid_neighbor_yes),
                   fun = ~as.character(cut(mean(.x), 
                                           col_breaks, 
                                           ordered_result = TRUE,
                                           labels = c("a", "b", "c", "d", "e")))) +
  geom_sf(fill = NA,
          color = "#444444",
          alpha = 0.3,
          size = 0.1) +
  scale_fill_manual(values = sf_map_colors,
                    labels = c("-2.0 to -0.5",
                               "-0.5 to -0.2",
                               "-0.2 to 0.2", 
                               "0.2 to 0.5", 
                               "0.5 to 2.0")) +
  labs(x = NULL, 
       y = NULL,
       fill = "Mean Residuals",
       title = "Model Residuals") +
  coord_sf(datum = NA)

```

<br>

Visual inspection suggests some minor clustering in our model errors. An empirical measure for the magnitude of this effect is the local Moran’s I statistic. Large or small values of this statistic suggest the presence of autocorrelation in that area.

<br>

```{r}
set.seed(451)

augmented_data_sampled <- augmented_data %>% 
  sample_n(3000)

col_breaks <- c(-10, -3, -1, 1, 3, 10)

augmented_data_sampled %>% 
  ggplot() +
  stat_summary_hex(aes(x = scene_gps_longitude, 
                       y = scene_gps_latitude,
                       z = morans_stat[,4]),
                   fun = ~as.character(cut(mean(.x), 
                                           col_breaks, 
                                           ordered_result = TRUE,
                                           labels = c("a", "b", "c", "d", "e")))) +
  geom_sf(fill = NA,
          color = "#444444",
          alpha = 0.3,
          size = 0.1) +
  scale_fill_manual(values = sf_map_colors,
                    labels = c("-10 to -3",
                               "-3 to -1",
                               "-1 to 1", 
                               "1 to 3", 
                               "3 to 10")) +
  labs(x = NULL, 
       y = NULL,
       fill = "Local Moran's I Statistic\nZ-Scores",
       title = "Spatial Autocorrelation") +
  coord_sf(datum = NA)
```

<br>

Since few areas have extreme values for the local Moran’s I statistic, we did not feel it necessary to generate a more complex model to address spatial autocorrelation. Our initial linear model suffered much more from spatial autocorrelation, which was our primary reason for abandoning it in favor of the neighborhood-level model. Evidently, the inclusion of neighborhoods was sufficient in accounting for much of the spatial autocorrelation present in the data.

<br>

#### Model Coefficients

Having selected the neighborhood level model as the appropriate model, we move on to interpretations of the resulting coefficients. The following plot displays the effects of each variable included in the model when compared to a reference patient (in this case, the reference is a black woman transported by ambulance with abuse of substance symptoms before COVID-19). The reference response time for this case is 9.9 minutes. As an example of coefficient interpretation, if the same woman instead had cardiovascular symptoms, but all other variables remained the same, we would expect an EMS unit to arrive in about 8.4 minutes, since the coefficient associated with cardiovascular symptoms is approximately 0.84.

<br>

```{r, fig.height = 12}
bayes_model_coefs_trans$neighbor_model_bayes_yes_interact %>%
  mutate(after_covid = str_detect(term, "(after_covidTRUE.*)")) %>%
  mutate(term = str_replace(term, "after_covidTRUE:", "")) %>%
  filter(!str_detect(term, r"(Sigma|b\[)")) %>% 
  left_join(term_dictionary, by = "term") %>% 
  filter(term_pretty != "Intercept") %>% 
  mutate(term_group = ifelse(str_detect(term, "patient"), 
                        " ", ifelse(str_detect(term, "possible"),
                                                       "  ",
                                                       ifelse(str_detect(term, "vehicle"),
                                                              "   ",
                                                              "    ")))) %>% 
  ggplot(aes(y = reorder(term_pretty, estimate_scale_factor), color = after_covid)) +
  geom_point(aes(x = estimate_scale_factor), size = 2.2) +
  geom_errorbar(aes(xmin = conf.low_scale_factor, xmax = conf.high_scale_factor), size = 1.1) +
  scale_color_manual(values = cbbPalette, labels = c("Before Covid-19", "During Covid-19")) +
  coord_cartesian(xlim = c(0.7, 1.3)) +
  geom_vline(xintercept = 1, alpha = 0.5) +
  labs(y = NULL, 
       x = glue("Scale Factor Compared to Reference Incident"),
       color = NULL,
       title = "Neighborhood Level Model Coefficients",
       caption = "Before COVID-19 is considered to be before March 15th. Point estimates are median estimates.\nIntervals are 95% credible intervals.") +
  theme(legend.position = "bottom") +
  facet_grid(term_group ~ ., 
             scales = "free_y",
             space = "free_y")

#ggsave(here::here("output", "coef_plot.png"), device = "png", height = 12, width = 15)
```

<br>

From this, we can gather that there is no significant difference in how long service takes across different races and genders. While this is not the only area of EMS services where equity issues can occur, it does not appear that there is systematic inequity in response times. 

However, we do see large differences in the response times for different types of symptoms. Some of these make sense, as a cardiovascular event is likely considered more time-sensitive than an alcohol or drug-related incident. We do see some variation in the response times to some symptoms across the beginning of the COVID-19 pandemic. Based on the plot above, we can be fairly confident that GI/GU cases take longer to serve during the COVID-19 era than before. Unfortunately, due to the relatively few incidents after March 15th, our estimates are not highly precise. For most of these symptoms, we cannot say for sure whether response times have increased or decreased. These results still serve to point out possible trends worth monitoring as the pandemic continues.

Because this is a neighborhood-level model, the expected change in response time due to being in a particular neighborhood is also interpretable (the interpretation of units is the same as explained above). When plotting response times across the region, we see that rural areas are expected to wait longer for emergency medical services than their urban counterparts. However, these results should be interpreted with caution because of our inability directly control for distance traveled. As neighborhood or census tract regions get larger, this deficiency in our model becomes more and more meaningful. Therefore, comparing smaller regions within Charlottesville to those in Albemarle is suspect. Still, differences within similarly-sized regions may be meaningful.

<br>

```{r, fig.height = 6}
map_colors <- colorBin(sf_map_colors, c(0.5, 2), c(0.65, 0.8, .95, 1.05, 1.30, 1.8))

bayes_model_coefs_trans$neighbor_model_bayes_yes_interact %>%
  mutate(term = str_replace(term, "after_covidTRUE:", "")) %>%
  filter(str_detect(term, r"(b\[)")) %>% 
  filter(!str_detect(term, "NEW_NAME")) %>% 
  mutate(term_pretty = str_replace(term, r"(b\[\(Intercept\) NAME:)", "")) %>%
  mutate(term_pretty = str_replace(term_pretty, r"(\])", "")) %>% 
  mutate(term_pretty = str_replace_all(term_pretty, r"(_)", " ")) %>% 
  mutate(NAME = term_pretty) %>% 
  mutate(term_pretty = str_replace(term_pretty, r"((\d\d\d))", r"(Census Tract \1)")) %>% 
  select(term_pretty, everything()) %>% 
  inner_join(prepared_data_sp, ., by = "NAME") %>% 
  group_by(NAME) %>% 
  slice_head(1) %>% 
  ungroup() %>% 
  leaflet(width = "100%") %>% 
  addMapboxGL(style = "mapbox://styles/mapbox/light-v9") %>%
  addPolygons(color = "#444444", weight = 1, smoothFactor = 0.5,
    opacity = 1.0, fillOpacity = 0.5,
    fillColor = ~map_colors(estimate_scale_factor),
    label = ~map(glue("{term_pretty}<br/>
                      Scale Factor Estimate: {round(estimate_scale_factor, 2)}<br/>
                      95% Credible Interval: ({round(conf.low_scale_factor, 2)}, {round(conf.high_scale_factor, 2)})"), htmltools::HTML)) %>% 
  addLegend("bottomright", pal = map_colors, values = ~estimate_scale_factor,
            title = htmltools::HTML("Response Time Scale Factor<br/>From Reference Case"),
            opacity = .8)
```

<br>

### Conclusions

While this analysis suffers from the lack of data to control for travel distance, it still tells us important information about the Charlottesville and Albemarle County emergency medical services. First, there do not appear to be any glaring disparities in response times across demographic groups.

Second, some types of symptoms are expected to be served much faster than others. For example, holding all other variables constant, we expect a cardiovascular incident to be responded to about 15% more quickly than a drug or alcohol related incident. With this information, EMS teams may be able to identify inconsistencies between response times and highly time-sensitive incident types, potentially increasing efficiency. Below we show our predicted range of response times for a cardiovascular incident and a drug or alcohol related incident. 

<br>

```{r}
set.seed(451)
test_cases <- tibble(after_covid = c(TRUE, TRUE),
                     patient_age = rep(45, 2),
                     patient_first_race_collapsed = c("black or african american", "black or african american"),
                     patient_gender = rep("female", 2),
                     possible_impression_category_collapsed = c("cv", "abuse of substance"),
                     time_of_day = rep(12, 2),
                     response_vehicle_type_collapsed = rep("ambulance", 2),
                     NAME = rep("Belmont", 2))

test_predictions <- posterior_predict(neighbor_model_bayes_yes_interact,
                                      newdata = test_cases,
                                      fun = exp)


colnames(test_predictions) <- c("test_1", "test_2")

test_predictions <- as_tibble(test_predictions) %>% 
  mutate(across(everything(), .fns = as.numeric))

test_predictions %>% 
  tidyr::pivot_longer(starts_with("test"),
               names_to = "case",
               values_to = "prediction") %>% 
  group_by(case) %>% 
  mutate(median = median(prediction)) %>% 
  ungroup() %>% 
  ggplot(aes(x = (prediction))) +
  geom_density(aes(color = case), fill = NA, size = 1.1) +
  geom_vline(aes(xintercept = median, color = case)) +
  coord_cartesian(xlim = c(0, 20)) +
  scale_color_manual(values = cbbPalette[c(3,7)],
                     labels = c("Cardiovascular Symptoms", "Abuse of Substance")) +
  labs(color = NULL, 
       x = "Predicted Response Time",
       y = "Density",
       caption = "Vertical Lines Represent Median Estimate") +
  theme(legend.position = "bottom")
               

```

<br>

Finally, COVID-19 has made calls take longer on average, although this increase is very small (on the order of 10 to 20 seconds) and is likely not medically significant. This change may be driven by the increased time required for EMS responders to adhere to stricter personal protective equipment guidelines. This hypothesis could be assessed with the ability to split response times into separate loadout time and travel time components.

<br>

### Next Steps

The most obvious improvement to this analysis would be the inclusion of travel distance from dispatch to incident location. Even with our attempts to account for it, this variable is likely impacting many of our estimates to some degree, and it would not be surprising to see results shift were we to include it in the model. Unfortunately, this data is not reliably collected, and while alternative approaches to estimate it certainly exist, implementing them ended up being outside the time frame of this summer project.

Including travel distance information would allow us to make more meaningful interpretation of neighborhood effects. As it stands, we assume that the neighborhood effects are capturing much of the travel distance information. It would be interesting to see if some neighborhoods are consistently under- or over-served. Of particular interest is the Pantops region, which has a large number of elderly residents at heightened risk of COVID-19. On the previous graphs, this region corresponds to census tract 105, just to the East of Charlottesville. Any changes in response times here would therefore be particularly consequential.

Finally, it is worth noting that response times are not the only way to assess medical service delivery, and expanding our analysis to other variables would help provide a more nuanced view of changes across the COVID-19 pandemic. For instance, drops in call volume (as discussed in the [background](https://dspg-young-scholars-program.github.io/dspg20CharlottesvilleEMSEquity/findings/context/) section of the site), may indicate certain communities who have become highly reluctant to rely on emergency services with potentially dangerous consequences. Other variables, like hospitals patients are transferred to, types of medical procedures implemented during calls, and more would also be relevant for further exploration.
